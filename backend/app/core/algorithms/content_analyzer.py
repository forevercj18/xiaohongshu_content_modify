"""
å†…å®¹åˆ†æå™¨ - è¿ç¦è¯æ£€æµ‹å’Œå†…å®¹è´¨é‡åˆ†æ
"""
import re
import jieba
from typing import List, Dict, Any
from sqlalchemy.orm import Session

from app.models.database import ProhibitedWord
from app.core.algorithms.smart_prohibited_detector import SmartProhibitedDetector


class ContentAnalyzer:
    """å†…å®¹åˆ†æå™¨"""
    
    def __init__(self, db: Session):
        self.db = db
        self.prohibited_words = self._load_prohibited_words()
        self.smart_detector = SmartProhibitedDetector(db)
    
    def _load_prohibited_words(self) -> List[Dict]:
        """åŠ è½½è¿ç¦è¯åº“"""
        words = self.db.query(ProhibitedWord).filter(ProhibitedWord.status == 1).all()
        return [
            {
                "word": word.word,
                "category": word.category,
                "risk_level": word.risk_level
            }
            for word in words
        ]
    
    async def analyze_content(self, content: str) -> Dict[str, Any]:
        """åˆ†æå†…å®¹"""
        # ä½¿ç”¨æ™ºèƒ½è¿ç¦è¯æ£€æµ‹å™¨
        detected_issues = self.smart_detector.detect_prohibited_words(content)
        
        # è®¡ç®—å†…å®¹è´¨é‡è¯„åˆ†
        content_score = self._calculate_content_score(content, detected_issues)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        suggestions = self._generate_suggestions(content, detected_issues)
        
        return {
            "issues": detected_issues,
            "score": content_score,
            "suggestions": suggestions
        }
    
    def _detect_prohibited_words(self, content: str) -> List[Dict]:
        """æ£€æµ‹è¿ç¦è¯"""
        issues = []
        
        for word_info in self.prohibited_words:
            word = word_info["word"]
            
            # ç²¾ç¡®åŒ¹é…
            for match in re.finditer(re.escape(word), content, re.IGNORECASE):
                issues.append({
                    "type": "prohibited_word",
                    "word": word,
                    "position": match.start(),
                    "risk_level": word_info["risk_level"],
                    "category": word_info["category"],
                    "suggestions": self._get_replacement_suggestions(word)
                })
            
            # å˜å½¢è¯æ£€æµ‹ï¼ˆç®€å•å®ç°ï¼‰
            variations = self._generate_word_variations(word)
            for variation in variations:
                for match in re.finditer(re.escape(variation), content, re.IGNORECASE):
                    issues.append({
                        "type": "prohibited_word_variation",
                        "word": variation,
                        "original_word": word,
                        "position": match.start(),
                        "risk_level": word_info["risk_level"],
                        "category": word_info["category"],
                        "suggestions": self._get_replacement_suggestions(word)
                    })
        
        return issues
    
    def _generate_word_variations(self, word: str) -> List[str]:
        """ç”Ÿæˆè¯æ±‡å˜å½¢ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰"""
        variations = []
        
        # æ·»åŠ ç©ºæ ¼å’Œç¬¦å·åˆ†éš”
        if len(word) > 1:
            for i in range(1, len(word)):
                variations.append(word[:i] + " " + word[i:])
                variations.append(word[:i] + "*" + word[i:])
                variations.append(word[:i] + "." + word[i:])
        
        return variations
    
    def _get_replacement_suggestions(self, word: str) -> List[str]:
        """è·å–æ›¿æ¢å»ºè®®ï¼ˆä»è°éŸ³è¯åº“è·å–ï¼‰"""
        suggestions = []
        
        # ä»è°éŸ³è¯åº“è·å–æ›¿æ¢å»ºè®®
        from app.models.database import OriginalWord, HomophoneReplacement
        
        original_word = self.db.query(OriginalWord).filter(
            OriginalWord.word == word,
            OriginalWord.status == 1
        ).first()
        
        if original_word:
            # è·å–è¯¥è¯çš„è°éŸ³æ›¿æ¢
            replacements = self.db.query(HomophoneReplacement).filter(
                HomophoneReplacement.original_word_id == original_word.id,
                HomophoneReplacement.status == 1
            ).order_by(
                HomophoneReplacement.priority.desc(),
                HomophoneReplacement.confidence_score.desc()
            ).limit(3).all()
            
            suggestions.extend([r.replacement_word for r in replacements])
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è°éŸ³è¯ï¼Œæä¾›åŸºç¡€æ›¿æ¢å»ºè®®
        if not suggestions and len(word) > 1:
            # ç¬¦å·åˆ†éš”
            suggestions.append(word[0] + "*" + word[1:])
            # ç©ºæ ¼åˆ†éš”
            suggestions.append(word[0] + " " + word[1:])
            # æ‹¼éŸ³æ›¿æ¢
            suggestions.append(word[0] + "x" + word[1:])
        
        return suggestions[:3]  # æœ€å¤šè¿”å›3ä¸ªå»ºè®®
    
    def _calculate_content_score(self, content: str, issues: List[Dict]) -> int:
        """è®¡ç®—å†…å®¹è´¨é‡è¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰"""
        base_score = 100
        
        # æ ¹æ®è¿ç¦è¯æ‰£åˆ†
        for issue in issues:
            risk_level = issue.get("risk_level", 1)
            if risk_level == 3:  # é«˜é£é™©
                base_score -= 20
            elif risk_level == 2:  # ä¸­é£é™©
                base_score -= 10
            else:  # ä½é£é™©
                base_score -= 5
        
        # å†…å®¹é•¿åº¦è¯„åˆ†
        content_length = len(content)
        if content_length < 50:
            base_score -= 10  # å†…å®¹å¤ªçŸ­
        elif content_length > 1000:
            base_score -= 5   # å†…å®¹è¿‡é•¿
        
        # æ®µè½ç»“æ„è¯„åˆ†
        paragraphs = content.split('\n')
        if len(paragraphs) == 1 and len(content) > 200:
            base_score -= 5  # ç¼ºå°‘æ®µè½åˆ†éš”
        
        return max(0, min(100, base_score))
    
    def _generate_suggestions(self, content: str, issues: List[Dict]) -> List[Dict]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # è¿ç¦è¯æ›¿æ¢å»ºè®®
        if issues:
            suggestions.append({
                "type": "prohibited_words",
                "title": "è¿ç¦è¯æ›¿æ¢",
                "description": f"å‘ç° {len(issues)} ä¸ªéœ€è¦æ›¿æ¢çš„è¯æ±‡ï¼Œå»ºè®®ä½¿ç”¨è°éŸ³è¯æˆ–å…¶ä»–è¡¨è¾¾æ–¹å¼ã€‚",
                "priority": "high"
            })
        
        # å†…å®¹é•¿åº¦å»ºè®®
        content_length = len(content)
        if content_length < 50:
            suggestions.append({
                "type": "content_length",
                "title": "å¢åŠ å†…å®¹é•¿åº¦",
                "description": "å†…å®¹è¾ƒçŸ­ï¼Œå»ºè®®å¢åŠ æ›´å¤šç»†èŠ‚å’Œæè¿°ï¼Œæå‡å†…å®¹ä¸°å¯Œåº¦ã€‚",
                "priority": "medium"
            })
        elif content_length > 1000:
            suggestions.append({
                "type": "content_length",
                "title": "ç²¾ç®€å†…å®¹",
                "description": "å†…å®¹è¾ƒé•¿ï¼Œå»ºè®®ç²¾ç®€è¡¨è¾¾ï¼Œçªå‡ºé‡ç‚¹ä¿¡æ¯ã€‚",
                "priority": "low"
            })
        
        # æ®µè½ç»“æ„å»ºè®®
        paragraphs = content.split('\n')
        if len(paragraphs) == 1 and len(content) > 200:
            suggestions.append({
                "type": "paragraph_structure",
                "title": "ä¼˜åŒ–æ®µè½ç»“æ„",
                "description": "å»ºè®®å°†é•¿æ®µè½åˆ†è§£ä¸ºå¤šä¸ªçŸ­æ®µè½ï¼Œæé«˜å¯è¯»æ€§ã€‚",
                "priority": "medium"
            })
        
        # è¡¨æƒ…ç¬¦å·å»ºè®®
        emoji_count = len(re.findall(r'[ğŸ˜€-ğŸ™]', content))
        if emoji_count == 0:
            suggestions.append({
                "type": "emoji",
                "title": "æ·»åŠ è¡¨æƒ…ç¬¦å·",
                "description": "é€‚å½“æ·»åŠ è¡¨æƒ…ç¬¦å·å¯ä»¥å¢åŠ å†…å®¹çš„äº²å’ŒåŠ›å’Œè¶£å‘³æ€§ã€‚",
                "priority": "low"
            })
        
        # äº’åŠ¨å…ƒç´ å»ºè®®
        question_count = content.count('?') + content.count('ï¼Ÿ')
        if question_count == 0:
            suggestions.append({
                "type": "interaction",
                "title": "å¢åŠ äº’åŠ¨å…ƒç´ ",
                "description": "æ·»åŠ é—®å¥å¯ä»¥æå‡ç”¨æˆ·äº’åŠ¨ï¼Œå¢åŠ è¯„è®ºå’Œç‚¹èµã€‚",
                "priority": "medium"
            })
        
        return suggestions